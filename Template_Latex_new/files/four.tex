%% Copyright 1998 Pepe Kubon
%%
%% `two.tex' --- 2nd chapter for thes-full.tex, thes-short-tex from
%%               the `csthesis' bundle
%%
%% You are allowed to distribute this file together with all files
%% mentioned in READ.ME.
%%
%% You are not allowed to modify its contents.
%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%     Chapter 2   
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{introduction}
\label{four}
\textbf{motivation , primarily problem to solve}\newline
we need jobs to match skills we need to detect skills\newline
\textbf{real motivation secondary problem that we are actually solving}\newline
how to detect skills. why it is challenging and different than usual keyword extraction problems 


\section{literature review}

\section{what are the algorithms in keyword extraction }
\textbf{NLP introduction} 
The automatic analysis of 

text involves a deep understanding of 

natural language by machines, a reality 

from which we are still very far off \cite{NLPSurvey}.
Hitherto, online information 

retrieval, aggregation, and processing 

have mainly been based on algorithms 

relying on the textual representation of 

web pages. Such algorithms are very 

good at retrieving texts, splitting them 

into parts, checking the spelling and 

counting the number of words. When 

it comes to interpreting sentences and 

extracting meaningful information, however, their capabilities are known to 

be very limited. NLp in fact, requires high level symbolic capabilities(Dyer 1994) including:

1-creation and propagation of dynamic 

bindings;
2-manipulation of recursive, constituent structures,
 acquisition and access of lexical, semantic, and episodic memories;
❏ control of multiple learning/process- ing modules and routing of informa- tion among such modules;
❏ grounding of basic-level language constructs (e.g., objects and actions) in perceptual/motor experiences;
❏ representation of abstract concepts.
All such capabilities are required to shift from mere NLP to what is usually referred to as natural language under- standing (Allen, 1987). Today, most of the existing approaches are still based on the syntactic representation of text, a method that relies mainly on word co- occurrence frequencies. Such algorithms are limited by the fact that they can pro- cess only the information that they can ‘see’. As human text processors, we do not have such limitations as every word we see activates a cascade of semantically related concepts, relevant episodes, and sensory experiences, all of which enable the completion of complex NLP tasks—such as word-sense disam- biguation, textual entailment, and semantic role labeling—in a quick and
effortless way.\cite{NLPSurvey}

\textbf{what is tagging?}
Tagging is the process of labeling web resources based on their content. Each label, or tag, corresponds to a topic in a given document. Unlike metadata assigned by authors, or by professional indexers in libraries, tags are assigned by end- users for organizing and sharing information that is of interest to them. The organic system of tags assigned by all users of a given web platform is called a folksonomy. \cite{folksonomy}

\textbf{what is keyphrase? what is keyphrase extraction?what is the goal of extracting keyphrases?}
Many journals ask their authors to provide a list of keywords for their articles. We call these keyphrases, rather than keywords, because they are often phrases of two or more words, rather than single words. We define a keyphrase list as a short list of phrases (typically five to fifteen noun phrases) that capture the main topics discussed in a given document. This paper is concerned with the automatic extraction of keyphrases from text.
Keyphrases are meant to serve multiple goals. For example, (1) when they are printed on the first page of a journal article, the goal is summarization. They enable the reader to quickly determine whether the given article is in the reader’s fields of interest. (2) When they are printed in the cumulative index for a journal, the goal is indexing. They enable the reader to quickly find a relevant article when the reader has a specific need. (3) When a search engine form has a field labelled keywords, the goal is to enable the reader to make the search more precise. A search for documents that match a given query term in the keyword field will yield a smaller, higher quality list of hits than a search for the same term in the full text of the documents. Keyphrases can serve these diverse goals and others, because the goals share the requirement for a short list of phrases that captures the main topics of the documents.
We define automatic keyphrase extraction as the automatic selection of important, topical phrases from within the body of a document. Automatic keyphrase extraction is a special case of the more general task of automatic keyphrase generation, in which the generated phrases do not necessarily appear in the body of the given document. 
\textbf{what is the differences between indexing and keyphrase list}

We discuss related work by other researchers in Section 3. The most closely related work involves the problem of automatic index generation (Fagan 1987, Salton 1988, Ginsberg 1993, Nakagawa 1997, Leung and Kan 1997). One difference between keyphrase extraction and index generation is that, although keyphrases may be used in an index, keyphrases have other applications, beyond indexing. Another difference between a keyphrase list and an index is length. Because a keyphrase list is relatively short, it must contain only the most important, topical phrases for a given document. Because an index is relatively long, it can contain many less important, less topical phrases. Also, a keyphrase list can be read and judged in seconds, but an index might never be read in its entirety. Automatic keyphrase extraction is thus a more demanding task than automatic index generation. \cite{turney2000}
\textbf{diffrence between keyphrase extraction and information extraction}
Keyphrase extraction is also distinct from information extraction, the task that has been studied in depth in the Message Understanding Conferences (MUC-3 1991, MUC-4 1992, MUC-5 1993, MUC-6 1995). Information extraction involves extracting specific types of task-dependent information. For example, given a collection of news reports on terrorist attacks, information extraction involves finding specific kinds of information, such as the name of the terrorist organization, the names of the victims, and the type of incident (e.g., kidnapping, murder, bombing). In contrast, keyphrase extraction is not specific. The goal in keyphrase extraction is to produce topical phrases, for any type of factual, prosaic document. \cite{turney2000}


in \cite{turney2000}  automatic keyphrase extraction is approach as a supervised learning task. We treat a document as a set of phrases, which must be classified as either positive or negative examples 
of keyphrases. This is the classical machine learning problem of learning from examples. In Section 5, we describe how we apply the C4.5 decision tree induction algorithm to this task (Quinlan 1993). There are several unusual aspects to this classification problem. For example, the positive examples constitute only 0.2% to 2.4% of the total number of examples. C4.5 is typically applied to more balanced class distributions.
The experiments in this paper use five collections of documents, with a combined total of 652 documents. The collections are presented in Section 4. In our first set of experiments (Section 6), we evaluate nine different ways to apply C4.5. In preliminary experiments with the training documents, we found that bagging seemed to improve the performance of C4.5 (Breiman 1996a, b, Quinlan 1996). Bagging works by generating many different decision trees and allowing them to vote on the classification of each example. We experimented with different numbers of trees and different techniques for sampling the training data. The experiments support the hypothesis that bagging improves the performance of C4.5 when applied to automatic keyphrase extraction.
During our experiments with C4.5, we came to believe that a specialized algorithm, developed specifically for learning to extract keyphrases, might achieve better results than a general-purpose learning algorithm, such as C4.5. Section 7 introduces the GenEx algorithm. GenEx is a hybrid of the Genitor steady-state genetic algorithm (Whitley 1989) and the Extractor parameterized keyphrase extraction algorithm (Turney 1997, 1999).4 Extractor works by assigning a numerical score to the phrases in the input document. The final output of Extractor is essentially a list of the highest scoring phrases. The behaviour of the scoring function is determined by a dozen numerical parameters. Genitor tunes the setting of these parameters, to maximize the performance of Extractor on a given set of training examples.
The second set of experiments (Section 8) supports the hypothesis that a specialized algorithm (GenEx) can generate better keyphrases than a general-purpose algorithm (C4.5). Both algorithms incorporate significant amounts of domain knowledge, but we avoided embedding specialized procedural knowledge in our application of C4.5. It appears that some degree of specialized procedural knowledge is necessary for automatic keyphrase extraction.
The third experiment (Section 9) looks at subjective human evaluation of the quality of the keyphrases produced by GenEx. On average, about 80% of the automatically generated keyphrases are judged to be acceptable and about 60% are judged to be good.\cite{turney2000}




\textbf{supervised} \newline
\textbf{unsupervised}\newline
\textbf{graph-based algorithms}\newline
\textbf{what works and what fails}\newline
\textbf {Automatic keyphrase extraction}
\section{our approach our algorithm}
\textbf{our algorithm a bit of introduction}\newline
\textbf{data acquisition}\newline
\textbf{data cleaning}\newline
\textbf{the results}\newline

\section{visualization of trends}

\section{conclusion}


